{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815474e1",
   "metadata": {},
   "source": [
    "### Description\n",
    "Multilingual NER model for the identification mountains.\n",
    "Supported languages: UA, EN\n",
    "\n",
    "### Overview\n",
    "This notebook demonstrates the inference capabilities of a BERT-based Named Entity Recognition (NER) model.\n",
    "It showcases:\n",
    "- Extracting entities from raw text.\n",
    "- Handling both English and Ukrainian inputs.\n",
    "- Analyzing performance on edge cases and ambiguous contexts.\n",
    "- Detailed metrics on the test set (quantitive evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cf5e2",
   "metadata": {},
   "source": [
    "### Setup & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from inference import MountainNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7586327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./mnt_ner_model to cuda...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./mnt_ner_model\"  # Adjust this path if needed!\n",
    "ner = MountainNER(model_path=model_path)\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4ec5e",
   "metadata": {},
   "source": [
    "### Visual inspection\n",
    "Below we test the model on specific example sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c8c8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(text, entities):\n",
    "    \"\"\"\n",
    "    Highlights entities using simple color codes (ANSI).\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        print(f\"{text} (No mountains found)\")\n",
    "        return\n",
    "\n",
    "    # ANSI escape codes for coloring\n",
    "    BLUE_BOLD = \"\\033[1;94m\"\n",
    "    RESET = \"\\033[0m\" # Reset bold colour\n",
    "\n",
    "    highlighted_text = text\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Wrap the found entity in color codes and brackets\n",
    "        replacement = f\"{BLUE_BOLD}[{entity}]{RESET}\"\n",
    "        highlighted_text = highlighted_text.replace(entity, replacement)\n",
    "\n",
    "    print(f\"Prediction: {highlighted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12e542f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Prediction:\n",
      "TOKEN           TAG\n",
      "------------------------------\n",
      "My              O\n",
      "dream           O\n",
      "is              O\n",
      "to              O\n",
      "climb           O\n",
      "Mount           B-MNT  <-- Found entity\n",
      "Everest         I-MNT  <-- Found entity\n",
      "and             O\n",
      "K2              B-MNT  <-- Found entity\n",
      "before          O\n",
      "I               O\n",
      "turn            O\n",
      "30              O\n",
      ".               O\n",
      "\n",
      "Cleaned version\n",
      "Prediction: My dream is to climb \u001b[1;94m[Mount Everest]\u001b[0m and \u001b[1;94m[K2]\u001b[0m before I turn 30.\n"
     ]
    }
   ],
   "source": [
    "# 1. Simple EN Test\n",
    "text_en = \"My dream is to climb Mount Everest and K2 before I turn 30.\"\n",
    "\n",
    "# We access [0] because predict returns a batch list\n",
    "raw_preds = ner.predict(text_en)[0]\n",
    "\n",
    "print(\"Raw Prediction:\")\n",
    "print(f\"{'TOKEN':<15} {'TAG'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Loop through and print nicely aligned\n",
    "for word, tag in raw_preds:\n",
    "    # Optional: Highlight the interesting tags to make them pop\n",
    "    if tag != 'O':\n",
    "        print(f\"{word:<15} {tag}  <-- Found entity\")\n",
    "    else:\n",
    "        print(f\"{word:<15} {tag}\")\n",
    "\n",
    "print(\"\\nCleaned version\")\n",
    "\n",
    "found_mountains = ner.clean_predictions(ner.predict(text_en))[0]\n",
    "visualize_predictions(text_en, found_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \u001b[1;94m[Говерла]\u001b[0m - найвища точка України, але \u001b[1;94m[Монблан]\u001b[0m вищий.\n"
     ]
    }
   ],
   "source": [
    "# 2. Multilingual Test\n",
    "text_ua = \"Говерла - найвища точка України, але Монблан вищий.\"\n",
    "\n",
    "found_mountains_ua = ner.clean_predictions(ner.predict(text_ua))[0]\n",
    "visualize_predictions(text_ua, found_mountains_ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96dfd1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Wikipedia Text Analysis (7 entities found) -\n",
      "Prediction: Heights of mountains are typically measured above sea level. \n",
      "Using this metric, \u001b[1;94m[Mount Everest]\u001b[0m is the highest mountain on Earth, at 8,848 metres (29,029 ft).[78] \n",
      "There are at least 100 mountains with heights of over 7,200 metres (23,622 ft) above sea level, all of which are located in central and southern Asia. \n",
      "The highest mountains above sea level are generally not the highest above the surrounding terrain. \n",
      "There is no precise definition of surrounding base, but \u001b[1;94m[Denali]\u001b[0m,[79] \u001b[1;94m[\u001b[1;94m[Mount Kilimanjaro]\u001b[0m]\u001b[0m and \u001b[1;94m[Nanga Parbat]\u001b[0m are possible candidates for the tallest mountain on land by this measure. \n",
      "The bases of mountain islands are below sea level, and given this consideration Mauna Kea (4,207 m (13,802 ft) above sea level) is the world's tallest mountain and volcano, rising about 10,203 m (33,474 ft) from the Pacific Ocean floor.[80]\n",
      "\n",
      "The highest mountains are not generally the most voluminous. \u001b[1;94m[Mauna Loa]\u001b[0m (4,169 m or 13,678 ft) is the largest mountain on Earth in terms of base area (about 2,000 sq mi or 5,200 km2) and volume (about 18,000 cu mi or 75,000 km3).[81] \n",
      "\u001b[1;94m[\u001b[1;94m[Mount Kilimanjaro]\u001b[0m]\u001b[0m is the largest non-shield volcano in terms of both base area (245 sq mi or 635 km2) and volume (1,150 cu mi or 4,793 km3). \n",
      "\u001b[1;94m[Mount Logan]\u001b[0m is the largest non-volcanic mountain in base area (120 sq mi or 311 km2).\n"
     ]
    }
   ],
   "source": [
    "wiki_text = \"\"\"Heights of mountains are typically measured above sea level. \n",
    "Using this metric, Mount Everest is the highest mountain on Earth, at 8,848 metres (29,029 ft).[78] \n",
    "There are at least 100 mountains with heights of over 7,200 metres (23,622 ft) above sea level, all of which are located in central and southern Asia. \n",
    "The highest mountains above sea level are generally not the highest above the surrounding terrain. \n",
    "There is no precise definition of surrounding base, but Denali,[79] Mount Kilimanjaro and Nanga Parbat are possible candidates for the tallest mountain on land by this measure. \n",
    "The bases of mountain islands are below sea level, and given this consideration Mauna Kea (4,207 m (13,802 ft) above sea level) is the world's tallest mountain and volcano, rising about 10,203 m (33,474 ft) from the Pacific Ocean floor.[80]\n",
    "\n",
    "The highest mountains are not generally the most voluminous. Mauna Loa (4,169 m or 13,678 ft) is the largest mountain on Earth in terms of base area (about 2,000 sq mi or 5,200 km2) and volume (about 18,000 cu mi or 75,000 km3).[81] \n",
    "Mount Kilimanjaro is the largest non-shield volcano in terms of both base area (245 sq mi or 635 km2) and volume (1,150 cu mi or 4,793 km3). \n",
    "Mount Logan is the largest non-volcanic mountain in base area (120 sq mi or 311 km2).\"\"\"\n",
    "\n",
    "# We get the first element [0] because our input is a single string\n",
    "wiki_found = ner.clean_predictions(ner.predict(wiki_text))[0]\n",
    "\n",
    "print(f\"- Wikipedia Text Analysis ({len(wiki_found)} entities found) -\")\n",
    "visualize_predictions(wiki_text, wiki_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a35167",
   "metadata": {},
   "source": [
    "While the model successfully identified complex entities like \"Mount Kilimanjaro\" and \"Mount Logan\", it is not perfect.\n",
    "\n",
    "Missed Entity: The model failed to identify the first appearance of *\"Mauna Kea\"* in the 6th sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa39aa",
   "metadata": {},
   "source": [
    "### Analysing text traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac62190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Футбольний клуб \"\u001b[1;94m[Карпати]\u001b[0m\" зіграв унічию сьогодні.\n"
     ]
    }
   ],
   "source": [
    "text_trap_ua = \"Футбольний клуб \\\"Карпати\\\" зіграв унічию сьогодні.\"\n",
    "\n",
    "found_ua = ner.clean_predictions(ner.predict(text_trap_ua))[0]\n",
    "visualize_predictions(text_trap_ua, found_ua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d33bb8",
   "metadata": {},
   "source": [
    "*Analysis: token overfitting*\n",
    "\n",
    "The model incorrectly identified **\"Карпати\"** (Football Club) as a mountain.\n",
    "During training, the token \"Карпати\" likely appeared exclusively with the MNT label\n",
    "\n",
    "To fix this, we must introduce negative sampling into the dataset (or improve prompts for generating dataset with LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd763bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from ./data/final/test.jsonl...\n",
      "Running inference on 569 examples...\n",
      "\n",
      "Overall Performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNT</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision  recall  f1-score  support\n",
       "MNT       0.92    0.91      0.91    137.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukrainian Performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNT</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision  recall  f1-score  support\n",
       "MNT       0.88    0.88      0.88     60.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English Performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNT</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision  recall  f1-score  support\n",
       "MNT       0.95    0.92      0.93     77.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = ner.evaluate_file(\"./data/final/test.jsonl\")\n",
    "\n",
    "# Display Overall Results\n",
    "print(\"\\nOverall Performance\")\n",
    "df_overall = pd.DataFrame(metrics['Overall']).transpose()\n",
    "display(df_overall.round(2))\n",
    "\n",
    "# Display Per-Language Results\n",
    "print(\"\\nUkrainian Performance\")\n",
    "if 'UA' in metrics:\n",
    "    df_ua = pd.DataFrame(metrics['UA']).transpose()\n",
    "    display(df_ua.round(2))\n",
    "else:\n",
    "    print(\"No UA data found.\")\n",
    "\n",
    "print(\"\\nEnglish Performance\")\n",
    "if 'EN' in metrics:\n",
    "    df_en = pd.DataFrame(metrics['EN']).transpose()\n",
    "    display(df_en.round(2))\n",
    "else:\n",
    "    print(\"No EN data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be805a0",
   "metadata": {},
   "source": [
    "### Conclusion & Analysis\n",
    "\n",
    "#### Performance Insights\n",
    "* The model is trustworthy (precision 0.95 in English) - rarely produces false positives.\n",
    "* The model performs better on English (F1 0.93) than on Ukrainian (F1 0.88). This is because BERT model was pre-trained on a significantly larger corpus of English text.\n",
    "* In Ukrainian, Precision and Recall are identical (0.88). This indicates the model is just as likely to miss a mountain as it is to hallucinate one.\n",
    "\n",
    "#### Error Insights\n",
    "* The model incorrectly identified Карпати as a mountain when it referred to the football club. (overfitted to the token, lack of negative samples)\n",
    "* The model missed an instance of \"Mauna Kea\" in wiki stress test. This proves that the model is imperfect.\n",
    "\n",
    "#### Conclusion\n",
    "1. Performance Summary\n",
    "* The model achieved an overall F1-score of 0.91 across 569 test entities. \n",
    "* English Performance (F1: 0.93): The model is highly reliable for English text.\n",
    "* Ukrainian Performance (F1: 0.88): While effective, the model exhibits a slight performance drop in Ukrainian.\n",
    "\n",
    "2. Key Strengths\n",
    "* High Precision (0.95 Overall): The model rarely produces false positives in standard contexts.\n",
    "* Long-Context Handling: The model successfully tracks multiple entities within dense paragraphs without losing coherence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
