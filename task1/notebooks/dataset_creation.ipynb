{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1813a4b",
   "metadata": {},
   "source": [
    "# Dataset creation notebook\n",
    "\n",
    "Purpose: create a dataset for NER model using Gemini API for batched based sample generation\n",
    "\n",
    "Structure:\n",
    "1. Setup & Configuration\n",
    "2. Bio tags assignment\n",
    "3. Generating and saving the whole dataset\n",
    "\n",
    "### Todo (future improvements)\n",
    "1. Implement checkpoint saves for dataset creation (in case of error)\n",
    "2. Tagging validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4739bc9",
   "metadata": {},
   "source": [
    "### Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f6d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b185c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_POSITIVE_EN = \"\"\"\n",
    "Generate 20 unique items in valid JSON Lines format. \n",
    "Output **only** JSON objects, one per line, with no additional commentary, explanation, or text. \n",
    "Each line must be a dictionary with keys:\n",
    "\"text\" — a sentence containing one or more specific mountain names\n",
    "\"entities\" — a list of all mountain names mentioned in the sentence, including alternative names\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_NEGATIVE_EN = \"\"\"\n",
    "Generate 20 unique items in valid JSON Lines format. \n",
    "Output **only** JSON objects, one per line, with no commentary.\n",
    "Each line must be a dictionary with:\n",
    "\"text\" — a sentence about geography, hiking, or nature\n",
    "\"entities\" — an empty list []\n",
    "Use generic terms such as \"hill\", \"ridge\", \"peak\", \"valley\", \"cliff\" and include real geographic names that are NOT mountains, like rivers, lakes, deserts, islands, or regions.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_POSITIVE_UA = \"\"\"\n",
    "Згенеруй 20 унікальних елементів у форматі JSON Lines.\n",
    "Виводь лише JSON об'єкти, 1 на рядок, без додаткових коментарів, пояснень або тексту. \n",
    "Кожен рядок має бути словником:\n",
    "\"text\": речення українською мовою, що містить назви однієї або кількох конкретних гір.\n",
    "\"entities\": список точних назв гір, що зустрічаються в реченні.\n",
    "Використовуй реальні гори (Карпати, Говерла, Гімалаї, Альпи тощо). \n",
    "\"\"\"\n",
    "\n",
    "PROMPT_NEGATIVE_UA = \"\"\"\n",
    "Згенеруй 20 унікальних елементів у форматі JSON Lines.\n",
    "Виводь лише JSON об'єкти, 1 на рядок, без додаткових коментарів, пояснень або тексту. \n",
    "Кожен рядок має бути словником:\n",
    "\"text\": речення українською про природу, географію.\n",
    "\"entities\": пустий список [].\n",
    "Використовуй загальні терміни, такі як «пагорб», «хребет», «вершина», «долина», «скеля», та вказуй справжні географічні назви, які НЕ є горами, наприклад, річки, озера, пустелі, острови чи регіони.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c2c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_api():\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = getpass(\"Enter Gemini API Key: \")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    return client\n",
    "\n",
    "# Maximum number of times to retry a failed API call before giving up on that specific request\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "def generate_batch_data(client, prompt, batch_count=1):\n",
    "    dataset = []\n",
    "\n",
    "    # Tqdm wrapper makes a progress bar\n",
    "    for i in tqdm(range(batch_count), desc=\"Generating batches\"):\n",
    "        try:\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    response = client.models.generate_content(\n",
    "                        model=\"gemini-2.5-flash\",\n",
    "                        contents=prompt,\n",
    "                        config=types.GenerateContentConfig(\n",
    "                            temperature=0.8,\n",
    "                            response_mime_type=\"application/json\"\n",
    "                        )\n",
    "                    )\n",
    "                    break # success\n",
    "                except Exception as e:\n",
    "                    # Exponential backoff (base 10)\n",
    "                    print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                    wait_time = 10 * (attempt + 1) \n",
    "                    print(f\"Retrying in {wait_time}s …\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "            # Extract the raw generated text\n",
    "            raw_text = response.candidates[0].content.parts[0].text\n",
    "\n",
    "            # Split by lines (JSONL format) and parse\n",
    "            batch_data = [json.loads(line) for line in raw_text.strip().splitlines() if line.strip()]\n",
    "\n",
    "            # Add to overall dataset\n",
    "            dataset.extend(batch_data)\n",
    "            \n",
    "            time.sleep(5)  # rate limit pause\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on batch {i}: {e}. Retrying after 10s …\")\n",
    "            time.sleep(10)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a653add",
   "metadata": {},
   "source": [
    "### Verify setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61120ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 1/1 [00:11<00:00, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples returned: 20\n",
      "Preview of first 2 samples sample:\n",
      "[{'text': 'Mount Everest, also known as Chomolungma, is the highest peak in the world.', 'entities': ['Mount Everest', 'Chomolungma']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = setup_api()\n",
    "    test_response = generate_batch_data(client, PROMPT_POSITIVE_EN, 1)\n",
    "    print(f\"Total samples returned: {len(test_response)}\")\n",
    "    print(\"Preview of first 2 samples sample:\")\n",
    "    print(test_response[:1])\n",
    "except Exception as e:\n",
    "    print(f\"Smoke test failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57537067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def assign_bio_tags(entry):\n",
    "    \"\"\"\n",
    "        Converts entity lists to BIO tags.\n",
    "    \"\"\"\n",
    "    text = entry['text']\n",
    "    \n",
    "    # Robust regex tokenization\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "    \n",
    "    entities = entry.get('entities', [])\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Tokenize the entity string using the exact same regex\n",
    "        entity_tokens = re.findall(r'\\w+|[^\\w\\s]', entity)\n",
    "        entity_len = len(entity_tokens)\n",
    "        \n",
    "        # Sliding window match\n",
    "        for i in range(len(tokens) - entity_len + 1):\n",
    "            if tokens[i:i+entity_len] == entity_tokens:\n",
    "                tags[i] = \"B-MNT\"\n",
    "                for j in range(1, entity_len):\n",
    "                    tags[i+j] = \"I-MNT\"\n",
    "    \n",
    "    language = entry.get(\"language\", None)\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": tags,\n",
    "        \"language\": language\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5272a54",
   "metadata": {},
   "source": [
    "### Bio-tags assignment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123f5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1:\n",
      "{\n",
      "    \"tokens\": [\n",
      "        \"The\",\n",
      "        \"climb\",\n",
      "        \"to\",\n",
      "        \"Mount\",\n",
      "        \"Fitz\",\n",
      "        \"Roy\",\n",
      "        \"was\",\n",
      "        \"difficult\",\n",
      "        \"near\",\n",
      "        \"K2\",\n",
      "        \"!\"\n",
      "    ],\n",
      "    \"ner_tags\": [\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"B-MNT\",\n",
      "        \"I-MNT\",\n",
      "        \"I-MNT\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"B-MNT\",\n",
      "        \"O\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Test 2:\n",
      "{\n",
      "    \"tokens\": [\n",
      "        \"The\",\n",
      "        \"river\",\n",
      "        \"flows\",\n",
      "        \"near\",\n",
      "        \"the\",\n",
      "        \"high\",\n",
      "        \"ridge\",\n",
      "        \",\",\n",
      "        \"far\",\n",
      "        \"from\",\n",
      "        \"the\",\n",
      "        \"city\",\n",
      "        \"center\",\n",
      "        \".\"\n",
      "    ],\n",
      "    \"ner_tags\": [\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\",\n",
      "        \"O\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Positive Sample\n",
    "test_data_1 = {\n",
    "    \"text\": \"The climb to Mount Fitz Roy was difficult near K2!\",\n",
    "    \"entities\": [\"Mount Fitz Roy\", \"K2\"]\n",
    "}\n",
    "\n",
    "# Test Case 2: Negative Sample\n",
    "test_data_2 = {\n",
    "    \"text\": \"The river flows near the high ridge, far from the city center.\",\n",
    "    \"entities\": []\n",
    "}\n",
    "\n",
    "# --- Execute Tests ---\n",
    "print(\"\\nTest 1:\")\n",
    "result_1 = assign_bio_tags(test_data_1)\n",
    "print(json.dumps(result_1, indent=4))\n",
    "\n",
    "print(\"\\nTest 2:\")\n",
    "result_2 = assign_bio_tags(test_data_2)\n",
    "print(json.dumps(result_2, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4f7ba",
   "metadata": {},
   "source": [
    "# Generating a whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae98d3",
   "metadata": {},
   "source": [
    "### Save & Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d2ecc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data, filename):\n",
    "    \"\"\"\n",
    "    Save jsonl data to file with proper UTF-8 encoding\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f, ensure_ascii=False)  # keep Cyrillic readable\n",
    "            f.write('\\n')\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"\n",
    "    Load jsonl data from file with proper UTF-8 encoding\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # avoid empty lines\n",
    "                data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45439df0",
   "metadata": {},
   "source": [
    "### Caution: This section generates data using an external API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ef978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Positive English Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 12/12 [02:35<00:00, 12.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Negative English Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 60/60 [11:30<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Positive Ukrainian Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 12/12 [03:02<00:00, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Negative Ukrainian Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating batches: 100%|██████████| 60/60 [15:34<00:00, 15.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate raw data\n",
    "\n",
    "# One batch contains 20 samples\n",
    "POSITIVE_EN_BATCH_COUNT = 12\n",
    "NEGATIVE_EN_BATCH_COUNT = 60\n",
    "POSITIVE_UA_BATCH_COUNT = 12\n",
    "NEGATIVE_UA_BATCH_COUNT = 60\n",
    "\n",
    "try:\n",
    "    client = setup_api()\n",
    "\n",
    "    print(\"\\nGenerating Positive English Samples...\")\n",
    "    pos_data_en = generate_batch_data(client, PROMPT_POSITIVE_EN, batch_count=POSITIVE_EN_BATCH_COUNT)\n",
    "    save_jsonl(pos_data_en, \"data/raw/raw_positive_en.jsonl\")\n",
    "\n",
    "    print(\"\\nGenerating Negative English Samples...\")\n",
    "    neg_data_en = generate_batch_data(client, PROMPT_NEGATIVE_EN, batch_count=NEGATIVE_EN_BATCH_COUNT)\n",
    "    save_jsonl(neg_data_en, \"data/raw/raw_negative_en.jsonl\")\n",
    "\n",
    "    print(\"\\nGenerating Positive Ukrainian Samples...\")\n",
    "    pos_data_ua = generate_batch_data(client, PROMPT_POSITIVE_UA, batch_count=POSITIVE_UA_BATCH_COUNT)\n",
    "    save_jsonl(pos_data_ua, \"data/raw/raw_positive_ua.jsonl\")\n",
    "\n",
    "    print(\"\\nGenerating Negative Ukrainian Samples...\")\n",
    "    neg_data_ua = generate_batch_data(client, PROMPT_NEGATIVE_UA, batch_count=NEGATIVE_UA_BATCH_COUNT)\n",
    "    save_jsonl(neg_data_ua, \"data/raw/raw_negative_ua.jsonl\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAPI generation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd283fbc",
   "metadata": {},
   "source": [
    "### Processing and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e549298",
   "metadata": {},
   "source": [
    "### Adding language labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_language_label(data, language):\n",
    "    for entry in data:\n",
    "        entry['language'] = language\n",
    "    return data\n",
    "\n",
    "# --- Adding labels ---\n",
    "en_data = load_jsonl(\"data/raw/raw_positive_en.jsonl\")\n",
    "en_data = add_language_label(en_data, \"EN\")\n",
    "save_jsonl(en_data, \"data/labeled/labeled_positive_en.jsonl\")\n",
    "\n",
    "en_data = load_jsonl(\"data/raw/raw_negative_en.jsonl\")\n",
    "en_data = add_language_label(en_data, \"EN\")\n",
    "save_jsonl(en_data, \"data/labeled/labeled_negative_en.jsonl\")\n",
    "\n",
    "ua_data = load_jsonl(\"data/raw/raw_positive_ua.jsonl\")\n",
    "ua_data = add_language_label(ua_data, \"UA\")\n",
    "save_jsonl(ua_data, \"data/labeled/labeled_positive_ua.jsonl\")\n",
    "\n",
    "ua_data = load_jsonl(\"data/raw/raw_negative_ua.jsonl\")\n",
    "ua_data = add_language_label(ua_data, \"UA\")\n",
    "save_jsonl(ua_data, \"data/labeled/labeled_negative_ua.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Raw Sample Count: 2880\n",
      "Unique Raw Sample Count: 2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying BIO Tags: 100%|██████████| 2844/2844 [00:00<00:00, 123670.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging complete. Total samples: 2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of labeled files\n",
    "files = [\n",
    "    \"data/labeled/labeled_positive_en.jsonl\",\n",
    "    \"data/labeled/labeled_negative_en.jsonl\",\n",
    "    \"data/labeled/labeled_positive_ua.jsonl\",\n",
    "    \"data/labeled/labeled_negative_ua.jsonl\"\n",
    "]\n",
    "\n",
    "# Load all datasets\n",
    "labeled_data = []\n",
    "for f in files:\n",
    "    labeled_data.extend(load_jsonl(f))\n",
    "\n",
    "random.shuffle(labeled_data)\n",
    "\n",
    "# DEDUPPLICATION\n",
    "print(f\"Original Raw Sample Count: {len(labeled_data)}\")\n",
    "\n",
    "unique_data = []\n",
    "processed_texts = set()\n",
    "\n",
    "for sample in labeled_data:\n",
    "    text = sample.get('text') \n",
    "    \n",
    "    if text and text not in processed_texts:\n",
    "        processed_texts.add(text)\n",
    "        unique_data.append(sample)\n",
    "\n",
    "raw_data = unique_data # Overwrite the raw_data list with unique samples\n",
    "\n",
    "print(f\"Unique Raw Sample Count: {len(raw_data)}\")\n",
    "\n",
    "processed_dataset = []\n",
    "for entry in tqdm(raw_data, desc=\"Applying BIO Tags\"):\n",
    "    try:\n",
    "        tagged_entry = assign_bio_tags(entry)\n",
    "        processed_dataset.append(tagged_entry)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Tagging failed for an entry: {e}\")\n",
    "        continue \n",
    "        \n",
    "print(f\"Tagging complete. Total samples: {len(processed_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166b577",
   "metadata": {},
   "source": [
    "### Split and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e82deb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SPLIT AND FINAL EXPORT\n",
    "train_data, remain_data = train_test_split(processed_dataset, test_size=0.4, random_state=42)\n",
    "val_data, test_data = train_test_split(remain_data, test_size=0.5, random_state=42)\n",
    "\n",
    "save_jsonl(train_data, \"data/final/train.jsonl\")\n",
    "save_jsonl(val_data, \"data/final/validation.jsonl\")\n",
    "save_jsonl(test_data, \"data/final/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
